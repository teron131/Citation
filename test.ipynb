{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"branch:verify_similarity:verify_similarity:{'write_query': 'write_query'}\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph\n\u001b[1;32m      3\u001b[0m full_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2305.05665v2.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIn multilingual neural machine translation, a similar phenomenon to the emergence behavior of IMAGEBIND is commonly observed and utilized: if languages are trained in the same latent space through learned implicit bridging, translation can be done between language pairs on which no paired data is provided\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1961\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1960\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1961\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1669\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m     get_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1671\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1672\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m     ):\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m output()\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/loop.py:425\u001b[0m, in \u001b[0;36mPregelLoop.tick\u001b[0;34m(self, input_keys)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# apply writes to managed values\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m mv_writes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# produce values output\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emit(\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_output_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys, writes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels\n\u001b[1;32m    429\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/loop.py:879\u001b[0m, in \u001b[0;36mSyncPregelLoop._update_mv\u001b[0;34m(self, key, values)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_mv\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, values: Sequence[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(cast(WritableManagedValue, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanaged\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mupdate, values)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"branch:verify_similarity:verify_similarity:{'write_query': 'write_query'}\""
          ]
        }
      ],
      "source": [
        "from graph import graph\n",
        "\n",
        "full_content = open(\"2305.05665v2.txt\").read()\n",
        "\n",
        "graph.invoke(\n",
        "    {\n",
        "        \"sentence\": \"In multilingual neural machine translation, a similar phenomenon to the emergence behavior of IMAGEBIND is commonly observed and utilized: if languages are trained in the same latent space through learned implicit bridging, translation can be done between language pairs on which no paired data is provided\",\n",
        "        \"full_content\": full_content,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "retriever = ArxivRetriever(top_k_results=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='In the past, the rapidly evolving field of sound classification greatly\n",
            "benefited from the application of methods from other domains. Today, we observe\n",
            "the trend to fuse domain-specific tasks and approaches together, which provides\n",
            "the community with new outstanding models.\n",
            "  In this work, we present an extension of the CLIP model that handles audio in\n",
            "addition to text and images. Our proposed model incorporates the ESResNeXt\n",
            "audio-model into the CLIP framework using the AudioSet dataset. Such a\n",
            "combination enables the proposed model to perform bimodal and unimodal\n",
            "classification and querying, while keeping CLIP's ability to generalize to\n",
            "unseen datasets in a zero-shot inference fashion.\n",
            "  AudioCLIP achieves new state-of-the-art results in the Environmental Sound\n",
            "Classification (ESC) task, out-performing other approaches by reaching\n",
            "accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.\n",
            "Further it sets new baselines in the zero-shot ESC-task on the same datasets\n",
            "(68.78% and 69.40%, respectively).\n",
            "  Finally, we also assess the cross-modal querying performance of the proposed\n",
            "model as well as the influence of full and partial training on the results. For\n",
            "the sake of reproducibility, our code is published.' metadata={'Entry ID': 'http://arxiv.org/abs/2106.13043v1', 'Published': datetime.date(2021, 6, 24), 'Title': 'AudioCLIP: Extending CLIP to Image, Text and Audio', 'Authors': 'Andrey Guzhov, Federico Raue, JÃ¶rn Hees, Andreas Dengel'}\n",
            "--------------------------------------------------\n",
            "page_content='An audio-visual event (AVE) is denoted by the correspondence of the visual\n",
            "and auditory signals in a video segment. Precise localization of the AVEs is\n",
            "very challenging since it demands effective multi-modal feature correspondence\n",
            "to ground the short and long range temporal interactions. Existing approaches\n",
            "struggle in capturing the different scales of multi-modal interaction due to\n",
            "ineffective multi-modal training strategies. To overcome this limitation, we\n",
            "introduce AVE-CLIP, a novel framework that integrates the AudioCLIP pre-trained\n",
            "on large-scale audio-visual data with a multi-window temporal transformer to\n",
            "effectively operate on different temporal scales of video frames. Our\n",
            "contributions are three-fold: (1) We introduce a multi-stage training framework\n",
            "to incorporate AudioCLIP pre-trained with audio-image pairs into the AVE\n",
            "localization task on video frames through contrastive fine-tuning, effective\n",
            "mean video feature extraction, and multi-scale training phases. (2) We propose\n",
            "a multi-domain attention mechanism that operates on both temporal and feature\n",
            "domains over varying timescales to fuse the local and global feature\n",
            "variations. (3) We introduce a temporal refining scheme with event-guided\n",
            "attention followed by a simple-yet-effective post processing step to handle\n",
            "significant variations of the background over diverse events. Our method\n",
            "achieves state-of-the-art performance on the publicly available AVE dataset\n",
            "with 5.9% mean accuracy improvement which proves its superiority over existing\n",
            "approaches.' metadata={'Entry ID': 'http://arxiv.org/abs/2210.05060v1', 'Published': datetime.date(2022, 10, 11), 'Title': 'AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization', 'Authors': 'Tanvir Mahmud, Diana Marculescu'}\n",
            "--------------------------------------------------\n",
            "page_content='Visual sound source localization poses a significant challenge in identifying\n",
            "the semantic region of each sounding source within a video. Existing\n",
            "self-supervised and weakly supervised source localization methods struggle to\n",
            "accurately distinguish the semantic regions of each sounding object,\n",
            "particularly in multi-source mixtures. These methods often rely on audio-visual\n",
            "correspondence as guidance, which can lead to substantial performance drops in\n",
            "complex multi-source localization scenarios. The lack of access to individual\n",
            "source sounds in multi-source mixtures during training exacerbates the\n",
            "difficulty of learning effective audio-visual correspondence for localization.\n",
            "To address this limitation, in this paper, we propose incorporating the text\n",
            "modality as an intermediate feature guide using tri-modal joint embedding\n",
            "models (e.g., AudioCLIP) to disentangle the semantic audio-visual source\n",
            "correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by\n",
            "predicting the class of sounding entities in mixtures. Subsequently, the\n",
            "textual representation of each sounding source is employed as guidance to\n",
            "disentangle fine-grained audio-visual source correspondence from multi-source\n",
            "mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables\n",
            "our framework to handle a flexible number of sources and exhibits promising\n",
            "zero-shot transferability to unseen classes during test time. Extensive\n",
            "experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets\n",
            "demonstrate significant performance improvements over state-of-the-art methods.\n",
            "Code is released at https://github.com/enyac-group/T-VSL/tree/main' metadata={'Entry ID': 'http://arxiv.org/abs/2404.01751v2', 'Published': datetime.date(2024, 7, 7), 'Title': 'T-VSL: Text-Guided Visual Sound Source Localization in Mixtures', 'Authors': 'Tanvir Mahmud, Yapeng Tian, Diana Marculescu'}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query = \"AudioCLIP adds audio as an additional modality into a CLIP framework, enabling zero-shot audio classification.\"\n",
        "docs = retriever.invoke(query)\n",
        "for doc in docs:\n",
        "    print(doc)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.711873196241966\n",
            "0.5347228688692579\n",
            "0.5064669280999857\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "\n",
        "def calculate_similarity(page_content: str, query: str) -> float:\n",
        "    doc_embedding = embeddings.embed_query(page_content)\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    similarity = cosine_similarity([doc_embedding], [query_embedding]).item()\n",
        "    return similarity\n",
        "\n",
        "\n",
        "for doc in docs:\n",
        "    print(calculate_similarity(doc.page_content, query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def is_arxiv_identifier(query: str) -> bool:\n",
        "    pattern = r\"\\d{2}(0[1-9]|1[0-2])\\.\\d{4,5}(v\\d+|)|\\d{7}.*\"\n",
        "    for item in query[:300].split():\n",
        "        if not re.match(pattern, item) or re.match(pattern, item).group(0) != item:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "is_arxiv_identifier(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['http://arxiv.org/pdf/2106.13043v1', 'http://arxiv.org/pdf/2210.05060v1', 'http://arxiv.org/pdf/2404.01751v2']\n"
          ]
        }
      ],
      "source": [
        "links = [doc.metadata[\"Entry ID\"].replace(\"/abs/\", \"/pdf/\") for doc in docs]\n",
        "print(links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5819485057638034\n",
            "0.5515498948932065\n",
            "0.4616582145934405\n"
          ]
        }
      ],
      "source": [
        "from langchain_playground.Tools import webloader\n",
        "\n",
        "content_list = []\n",
        "for link in links:\n",
        "    content = webloader(link)\n",
        "    content_list.append(content)\n",
        "\n",
        "\n",
        "for content in content_list:\n",
        "    print(calculate_similarity(content, query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Could not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention': [1/2] /usr/local/cuda-12.3/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/TH -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/teron/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
            "\u001b[31mFAILED: \u001b[0mms_deform_attn_cuda.cuda.o \n",
            "/usr/local/cuda-12.3/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/TH -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/teron/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu:19:9: warning: #pragma once in main file\n",
            "   19 | #pragma once\n",
            "      |         ^~~~\n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu:19:9: warning: #pragma once in main file\n",
            "   19 | #pragma once\n",
            "      |         ^~~~\n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu(69): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n",
            "         ; at::ScalarType _st = ::detail::scalar_type(the_type); ; switch (_st) { case at::ScalarType::Double: { do { if constexpr (!at::should_include_kernel_dtype( at_dispatch_name, at::ScalarType::Double)) { if (!(false)) { ::c10::detail::torchCheckFail( __func__, \"/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\", static_cast<uint32_t>(69), (::c10::detail::torchCheckMsgImpl( \"Expected \" \"false\" \" to be true, but got false.  \" \"(Could this error message be improved?  If so, \" \"please report an enhancement request to PyTorch.)\", \"dtype '\", toString(at::ScalarType::Double), \"' not selected for kernel tag \", at_dispatch_name))); }; } } while (0); using scalar_t [[maybe_unused]] = c10::impl::ScalarTypeToCPPTypeT<at::ScalarType::Double>; return \n",
            "                                                      ^\n",
            "\n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu(140): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n",
            "         ; at::ScalarType _st = ::detail::scalar_type(the_type); ; switch (_st) { case at::ScalarType::Double: { do { if constexpr (!at::should_include_kernel_dtype( at_dispatch_name, at::ScalarType::Double)) { if (!(false)) { ::c10::detail::torchCheckFail( __func__, \"/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\", static_cast<uint32_t>(140), (::c10::detail::torchCheckMsgImpl( \"Expected \" \"false\" \" to be true, but got false.  \" \"(Could this error message be improved?  If so, \" \"please report an enhancement request to PyTorch.)\", \"dtype '\", toString(at::ScalarType::Double), \"' not selected for kernel tag \", at_dispatch_name))); }; } } while (0); using scalar_t [[maybe_unused]] = c10::impl::ScalarTypeToCPPTypeT<at::ScalarType::Double>; return \n",
            "                                                      ^\n",
            "\n",
            "2 errors detected in the compilation of \"/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\".\n",
            "ninja: build stopped: subcommand failed.\n",
            "\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from langchain_playground.Tools import webloader\n",
        "\n",
        "content = webloader(\"https://arxiv.org/pdf/2305.05665v2\")\n",
        "with open(\"2305.05665v2.txt\", \"w\") as f:\n",
        "    f.write(content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
