{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence': 'In multilingual neural machine translation, a similar phenomenon to the emergence behavior of IMAGEBIND is commonly observed and utilized: if languages are trained in the same latent space through learned implicit bridging, translation can be done between language pairs on which no paired data is provided',\n",
              " 'full_content': \"## IMAGEBIND: One Embedding Space To Bind Them All\\n\\nRohit Girdhar ∗ Alaaeldin El-Nouby ∗ Zhuang Liu Mannat Singh Kalyan Vasudev Alwala Armand Joulin Ishan Misra ∗\\n\\n## FAIR, Meta AI\\n\\nhttps://facebookresearch.github.io/ImageBind\\n\\nFigure 1. IMAGEBIND's joint embedding space enables novel multimodal capabilities. By aligning six modalities' embedding into a common space, IMAGEBIND enables: 1) Cross-Modal Retrieval, which shows emergent alignment of modalities such as audio, depth or text, that aren't observed together. 2) Adding embeddings from different modalities naturally composes their semantics. And 3) Audio-toImage generation, by using our audio embeddings with a pre-trained DALLE-2 decoder designed to work with CLIP text embeddings.\\n\\n<!-- image -->\\n\\n## Abstract\\n\\nWe present IMAGEBIND , an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. IMAGEBIND can leverage recent large scale vision-language models, and extends their zeroshot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-theart on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that IMAGEBIND serves as a new way to evaluate vision models for visual and non-visual tasks.\\n\\n## 1. Introduction\\n\\nA single image can bind together many experiences - an image of a beach can remind us of the sound of waves, the texture of the sand, a breeze, or even inspire a poem. This 'binding' property of images offers many sources of supervision to learn visual features, by aligning them with any of the sensory experiences associated with images. Ideally, for a single joint embedding space, visual features should be learned by aligning to all of these sensors. However, this requires acquiring all types and combinations of paired data with the same set of images, which is infeasible.\\n\\nRecently, many methods learn image features aligned with text [1, 31, 46, 60, 64, 65, 82, 83], audio [3, 4, 50, 55, 56, 70] etc . These methods use a single pair of modalities or, at best, a few visual modalities. However, the final embeddings are limited to the pairs of modalities used for training. Thus, video-audio embeddings cannot directly be used for image-text tasks and vice versa. A major obstacle in learning a true joint embedding is the absence of large quantities of multimodal data where all modalities are present together.\\n\\nIn this paper, we present IMAGEBIND , which learns a single shared representation space by leveraging multiple types of image-paired data. It does not need datasets where all modalities co-occur with each other. Instead, we leverage the binding property of images and we show that just aligning each modality's embedding to image embeddings leads to an emergent alignment across all of the modalities. In practice, IMAGEBIND leverages web-scale (image, text) paired data and combines it with naturally occurring paired data such as (video, audio), (image, depth) etc . to learn a single joint embedding space. This allows IMAGEBIND to implicitly align the text embeddings to other modalities such as audio, depth etc ., enabling zero-shot recognition capabilities on that modality without explicit semantic or textual pairing. Moreover, we show that it can be initialized with large-scale vision-language models such as CLIP, thereby leveraging the rich image and text representations of these models. Thus, IMAGEBIND can be applied to a variety of different modalities and tasks with little training.\\n\\nWeuse large-scale image-text paired data along with naturally paired 'self-supervised' data across four new modalities - audio, depth, thermal, and Inertial Measurement Unit (IMU) readings - and show strong emergent zero-shot classification and retrieval performance on tasks for each of these modalities. These emergent properties improve as the underlying image representation is made stronger. On audio classification and retrieval benchmarks, IMAGEBIND's emergent zero-shot classification matches or outperforms specialist models trained with direct audio-text supervision on benchmarks like ESC, Clotho, AudioCaps. IMAGEBIND representations also outperform specialist supervised models on few-shot evaluation benchmarks. Finally, we show that IMAGEBIND's joint embeddings can be used for a wide variety of compositional tasks as illustrated in Figure 1, including cross-modal retrieval, combining embeddings via arithmetic, detecting audio sources in images, and generating images given audio input.\\n\\n## 2. Related Work\\n\\nIMAGEBIND builds upon several advances in visionlanguage, multimodal, and self-supervised research.\\n\\nLanguage Image Pre-training. Training images jointly with linguistic signals like words or sentences has been shown to be an effective method for zero-shot, openvocabulary recognition and text to image retrieval [14, 18, 38, 68]. Language as supervision can further be used for learning strong video representations [2, 47, 48]. Joulin et al . show that using large-scale image dataset with noisy captions yields strong visual features. Recently, CLIP, ALIGN and Florence collect large collections of image and text pairs and train models to embed image and language inputs in a joint space using contrastive learning, exhibiting impressive zero-shot performance. CoCa\\n\\nadds an image captioning objective on top of the contrastive loss for improved performance. Flamingo handles arbitrarily interleaved images and texts, and achieves state of the art on many few-shot learning benchmarks. LiT adopts contrastive training for fine-tuning and observes freezing image encoders works the best. This prior line of works mostly considers image and text, while our work enables zero-shot recognition on multiple modalities.\\n\\nMulti-Modal Learning. Our work binds multiple modality representations in a joint embedding space. Prior works explored joint training of multiple modalities in a supervised [21, 42] or self-supervised contexts [3, 20, 50, 70, 74]. The success of image and language pre-training methods such as CLIP has inspired approaches that revisits learning deep semantic representations through matching other modalities with linguistic inputs. Various methods adapt CLIP to extract semantically strong video representations [15, 43, 45, 79]. Most related to our method, Nagrani et al . create a weakly-labeled dataset for paired videoaudio and captions that allows for training multi-modal video-audio encoder to match textual features resulting in strong audio and video retrieval and captioning performance. AudioCLIP adds audio as an additional modality into a CLIP framework, enabling zero-shot audio classification. In contrast, IMAGEBIND does not require explicit paired data between all modalities and instead leverages image as a natural weak supervision for unifying modalities.\\n\\nFeature Alignment Pre-trained CLIP models have been utilized as teachers to supervise other models due to the strength of its visual representations [44, 58, 75]. Moreover, CLIP joint image and text embedding space has also been leveraged for a variety of zero-shot tasks like detection [24, 88], segmentation, mesh animation etc . showing the power of joint embedding spaces. PointCLIP finds a pre-trained CLIP encoder can be used for 3D recognition by projecting a point cloud to a number of 2D depth map views, which in turn are encoded using CLIP visual encoder. In multilingual neural machine translation, a similar phenomenon to the emergence behavior of IMAGEBIND is commonly observed and utilized: if languages are trained in the same latent space through learned implicit bridging, translation can be done between language pairs on which no paired data is provided [33, 40].\\n\\n## 3. Method\\n\\nOur goal is to learn a single joint embedding space for all modalities by using images to bind them together. We align each modality's embedding to image embeddings, such as text to image using web data and IMU to video using video data captured from egocentric cameras with IMU. We show that the resulting embedding space has a powerful emergent zero-shot behavior that automatically associates pairs of modalities without seeing any training data for that spe-\\n\\nFigure 2. IMAGEBIND overview. Different modalities occur naturally aligned in different data sources, for instance images+text and video+audio in web data, depth or thermal information with images, IMU data in videos captured with egocentric cameras, etc . IMAGEBIND links all these modalities in a common embedding space, enabling new emergent alignments and capabilities.\\n\\n<!-- image -->\\n\\ncific pair. We illustrate our approach in Figure 2.\\n\\n## 3.1. Preliminaries\\n\\nAligning specific pairs of modalities. Contrastive learning is a general technique for learning an embedding space by using pairs of related examples (positives) and unrelated examples (negatives). Using pairs of aligned observations, contrastive learning can align pairs of modalities such as (image, text), (audio, text), (image, depth), (video, audio) etc . However, in each case, the joint embeddings are trained and evaluated using the same pairs of modalities. Thus, (video, audio) embeddings are not directly applicable for text-based tasks while (image, text) embeddings cannot be applied for audio tasks.\\n\\nZero-shot image classification using text prompts. CLIP popularized a 'zero-shot' classification task based on an aligned (image, text) embedding space. This involves constructing a list of text descriptions that describe the classes in a dataset. An input image is classified based on its similarity to the text descriptions in the embedding space. Unlocking such zero-shot classification for other modalities requires specifically training using paired text data, e.g ., (audio, text) or (point-clouds, text). In contrast, IMAGEBIND unlocks zero-shot classification for modalities without paired text data.\\n\\n## 3.2. Binding modalities with images\\n\\nIMAGEBIND uses pairs of modalities ( I M , ), where I represents images and M is another modality, to learn a single joint embedding. We use large-scale web datasets with (image, text) pairings that span a wide range of semantic concepts. Additionally, we use the natural, self-supervised pairing of other modalities - audio, depth, thermal, and Intertial Measurement Unit (IMU) - with images.\\n\\nConsider the pair of modalities ( I M , ) with aligned observations. Given an image I i and its corresponding observation in the other modality M i , we encode them into normalized embeddings: q i = f ( I i ) and k i = g ( M i ) where f, g are deep networks. The embeddings and the encoders\\n\\nare optimized using an InfoNCE loss:\\n\\n̸\\n\\n<!-- formula-not-decoded -->\\n\\n̸\\n\\nwhere τ is a scalar temperature that controls the smoothness of the softmax distribution and j denotes unrelated observations, also called 'negatives'. We follow and consider every example j = i in the mini-batch to be a negative. The loss makes the embeddings q i and k i closer in the joint embedding space, and thus aligns I and M . In practice, we use a symmetric loss L I M , + L MI , .\\n\\nEmergent alignment of unseen pairs of modalities. IMAGEBIND uses modalities paired with images, i.e ., pairs of the form ( I M , ) to align each the embeddings from each modality M to those from images. We observe an emergent behavior in the embedding space that aligns two pairs of modalities ( M M 1 , 2 ) even though we only train using the pairs ( I M , 1 ) and ( I M , 2 ) . This behavior allows us to perform a wide variety of zero-shot and cross-modal retrieval tasks without training for them. We achieve stateof-the-art zero-shot text-audio classification results without observing a single sample of paired (audio, text).\\n\\n## 3.3. Implementation Details\\n\\nIMAGEBIND is conceptually simple and can be implemented in many different ways. We deliberately choose a vanilla implementation that is flexible and allows for an effective study and easy adoption. In § 5, we present design decisions that are critical for good emergent 'binding'.\\n\\nEncoding modalities. We use a Transformer architecture for all the modality encoders. We use the Vision Transformer (ViT) for images. Following, we use the same encoder for images and videos. We temporally inflate the patch projection layer of the ViT and use 2 frame video clips sampled from 2 seconds. We follow for encoding audio and convert a 2 second audio sampled at 16kHz into spectrograms using 128 mel-spectrogram bins. As the spectrogram is also a 2D signal like an image, we use a ViT with a patch size of 16 and stride 10 . We treat thermal images and depth images as one-channel images and\\n\\nTable 1. Emergent zero-shot classification datasets for audio, depth, thermal, and Inertial Measurement Unit (IMU) modalities. We evaluate IMAGEBIND without training for any of these tasks and without training on paired text data for these modalities. For each dataset, we report the task (classification or retrieval), number of classes (#cls), metric for evaluation (Accuracy or mean Average Precision), and the number of test samples (#test).\\n\\n| Dataset                         | Task          |     | #cls Metric   |   #test |\\n|---------------------------------|---------------|-----|---------------|---------|\\n| Audioset Audio-only (AS-A) | Audio cls.    | 527 | mAP           |   19048 |\\n| ESC 5-folds (ESC)          | Audio cls.    | 50  | Acc           |     400 |\\n| Clotho (Clotho)            | Retrieval     | -   | Recall        |    1045 |\\n| AudioCaps (AudioCaps)      | Retrieval     | -   | Recall        |     796 |\\n| VGGSound (VGGS)             | Audio cls.    | 309 | Acc           |   14073 |\\n| SUN Depth-only (SUN-D)     | Scene cls.    | 19  | Acc           |    4660 |\\n| NYU-v2 Depth-only (NYU-D)  | Scene cls.    | 10  | Acc           |     653 |\\n| LLVIP (LLVIP)              | Person cls.   | 2   | Acc           |   15809 |\\n| Ego4D (Ego4D)              | Scenario cls. | 108 | Acc           |   68865 |\\n\\nalso use a ViT to encode them. We follow to convert depth into disparity maps for scale invariance. We extract the IMU signal consisting of accelerometer and gyroscope measurements across the X Y , , and Z axes. We use 5 second clips resulting in 2K time step IMU readings which are projected using a 1D convolution with a kernel size of 8 . The resulting sequence is encoded using a Transformer. Finally, we follow the text encoder design from CLIP.\\n\\nWe use separate encoders for images, text, audio, thermal images, depth images, and IMU. We add a modalityspecific linear projection head on each encoder to obtain a fixed size d dimensional embedding, that is normalized and used in the InfoNCE loss from Eq 1. In addition to ease of learning, this setup allows us to also initialize a subset of the encoders using pretrained models, e.g ., the image and text encoder using CLIP or OpenCLIP.\\n\\n## 4. Experiments\\n\\nWe first describe the main experimental setup and provide full details in the supplement.\\n\\nNaturally paired modalities and datasets. We use IMAGEBIND on six modalities -image/video, text, audio, depth, thermal images, and IMU. As described in § 3.3, we treat videos as 2 frame images and process them the same as images. For the naturally available paired data, we use the (video, audio) pairs from the Audioset dataset, (image, depth) pairs from the SUN RGB-D dataset, (image, thermal) pairs from the LLVIP dataset and (video, IMU) pairs from the Ego4D dataset. For these pairs of modalities, we do not use any extra supervision like class labels, text etc . Since SUN RGB-D and LLVIP are relatively small, we follow and replicate them 50 × for training.\\n\\nLarge scale image-text pairs. We leverage image-text supervision from large-scale web data. For ease of experimentation, we use pretrained models that are trained on billions of (image, text) pairs. Specifically, we use the\\n\\npretrained vision (ViT-H 630M params) and text encoders (302M params) from OpenCLIP [11, 30].\\n\\nEncoders for each modality. We convert audio into 2D mel-spectrograms, and thermal and depth modalities into 1 channel images and use ViT-B, ViT-S encoders respectively. The image and text encoders are kept frozen during the IMAGEBIND training and the audio, depth, thermal, and IMU encoders are updated.\\n\\nEmergent zero-shot vs . zero-shot. Methods such as CLIP, AudioCLIP etc . train with modality pairs, (image, text) and (audio, text), to demonstrate zero-shot classification using text-prompts for the same modality. In contrast, IMAGEBIND binds modalities together using only image-paired data. Thus, just by training on (image, text) and (image, audio), IMAGEBIND can perform zero-shot classification of audio using text prompts. As we do not directly train for this ability, we term it emergent zero-shot classification to distinguish it from methods that specifically train using paired text-supervision for all modalities.\\n\\nEvaluation on downstream tasks. We comprehensively evaluate IMAGEBIND on a many different downstream tasks using different protocols. We summarize the main datasets used for evaluation in Table 1.\\n\\n## 4.1. Emergent zero-shot classification\\n\\nWe evaluate IMAGEBIND on emergent zero-shot classification and use the text prompt templates from (full details in Appendix B). We report the results in Table 2. Each task measures IMAGEBIND's ability to associate text embeddings to the other modalities without observing them together during training. Given the novelty of our problem setting, there are no 'fair' baselines to compare IMAGEBIND with. Nevertheless, we compare to prior work that uses text paired with certain modalities ( e.g . audio [27, 51]), and for certain 'visual-like' modalities such as depth and thermal, we use the CLIP model directly. We also report the best reported supervised upper bound per benchmark.\\n\\nIMAGEBIND achieves a high emergent zero-shot classification performance. On each benchmark, IMAGEBIND achieves strong gains and even compares favorably to supervised specialist models trained for the specific modality and task. These results demonstrate that IMAGEBIND aligns the modalities and implicitly transfers the text supervision associated with images to other modalities like audio. In particular, IMAGEBIND shows strong alignment for non-visual modalities like audio and IMU suggesting that their naturally available pairing with images is a powerful source of supervision. For completeness, we also report the standard zero-shot image (ImageNet - IN1K, Places-365 P365) and video (Kinetics400 - K400, MSR-VTT 1kA - MSR-VTT) tasks. As the image &amp; text encoders are initialized (and frozen) using OpenCLIP, these results match those of OpenCLIP.\\n\\nTable 2. Emergent zero-shot classification of IMAGEBIND using text prompts highlighted in blue. IMAGEBIND aligns images with text, depth, audio, thermal and IMU modalities. The resulting embedding space can associate text embeddings with the non-image modalities, and leads to strong emergent zero-shot classification. We show strong performance even on non-visual modalities such as audio and IMU. We compare to 'Text Paired' baselines wherever possible, which trains with paired text data for that modality. ∗ We use the OpenCLIP ViTH[30] on depth rendered as grayscale images. † that uses AS class names as supervision during training, and hence is not 'zero-shot'. Overall, IMAGEBIND shows strong emergent zero-shot performance, even compared to such upper bounds. We also report the absolute state-of-the-art (SOTA) on each dataset for reference, which typically uses additional supervision, model ensembles etc . We report the top-1 classification accuracy for all datasets except MSR-VTT (Recall@1) and Audioset Audio-only (mAP).\\n\\n|               | IN1K      | P365      | K400      | MSR-VTT   | NYU-D     | SUN-D     | AS-A        | VGGS      | ESC         | LLVIP   | Ego4D   |\\n|---------------|-----------|-----------|-----------|-----------|-----------|-----------|-------------|-----------|-------------|---------|---------|\\n| Random        | 0.1       | 0.27      | 0.25      | 0.1       | 10.0      | 5.26      | 0.62        | 0.32      | 2.75        | 50.0    | 0.9     |\\n| IMAGEBIND     | 77.7      | 45.4      | 50.0      | 36.1      | 54.0      | 35.1      | 17.6        | 27.8      | 66.9        | 63.4    | 25.0    |\\n| Text Paired   | -         | -         | -         | -         | 41.9 ∗    | 25.4 ∗    | 28.4 † | -         | 68.6 † | -       | -       |\\n| Absolute SOTA | 91.0 | 60.7 | 89.9 | 57.7 | 76.7 | 64.9 | 49.6   | 52.5 | 97.0    | -       | -       |\\n\\nTable 3. Emergent zero-shot audio retrieval and classification.\\n\\n|                                                  | Emergent   | Clotho   | Clotho    | AudioCaps   | AudioCaps               | ESC   |\\n|--------------------------------------------------|------------|----------|-----------|-------------|-------------------------|-------|\\n|                                                  |            |          |           |             | R@1 R@10 R@1 R@10 Top-1 |       |\\n| Uses audio and text supervision AudioCLIP ✗ |            |          |           |             |                         | 68.6  |\\n| Uses audio and text loss AVFIC              | ✗          | 3.0      | 17.5      | 8.7         | 37.7                    |       |\\n| No audio and text supervision IMAGEBIND          | ✓          | 6.0      | 28.4      | 9.3         | 42.3                    | 66.9  |\\n| Supervised AVFIC finetuned ARNLQ       | ✗ ✗        | 8.4 12.6 | 38.6 45.4 | 24.3        | 72.1                    |       |\\n\\nWe compare IMAGEBIND to prior work on zero-shot audio retrieval and audio classification. Without using audio-specific supervision, IMAGEBIND outperforms prior methods on zero-shot retrieval and has comparable performance on the classification task. IMAGEBIND's emergent zero-shot performance approaches those of specialist supervised models.\\n\\n## 4.2. Comparison to prior work\\n\\nWe now compare IMAGEBIND against prior work in zero-shot retrieval and classification tasks.\\n\\nZero-shot text to audio retrieval and classification. Unlike IMAGEBIND, prior work trains using paired data for that modality, e.g ., AudioCLIP uses (audio, text) supervision and AVFIC uses automatically mined (audio, text) pairs. We compare their zero-shot text to audio retrieval and classification performance to IMAGEBIND's emergent retrieval and classification in Table 3.\\n\\nIMAGEBIND significantly outperforms prior work on the audio text retrieval benchmarks. On the Clotho dataset, IMAGEBIND has double the performance of AVFIC despite not using any text pairing for audio during training. Compared to the supervised AudioCLIP model, IMAGEBIND achieves comparable audio classification performance on ESC. Note that AudioCLIP uses class names from AudioSet as text targets for audio-text training, hence is referred to as 'su-\\n\\nTable 4. Zero-shot text based retrieval on MSR-VTT 1K-A. We compare IMAGEBIND's emergent retrieval performance using audio and observe that it performs favorably to methods that use the stronger video modality for retrieval.\\n\\n|                 | Modality   | Emergent   | MSR-VTT   | MSR-VTT   | MSR-VTT      |\\n|-----------------|------------|------------|-----------|-----------|--------------|\\n|                 |            |            |           |           | R@1 R@5 R@10 |\\n| MIL-NCE    | V          | ✗          | 8.6       | 16.9      | 25.8         |\\n| SupportSet | V          | ✗          | 10.4      | 22.2      | 30.0         |\\n| FIT         | V          | ✗          | 15.4      | 33.6      | 44.1         |\\n| AVFIC      | A+V        | ✗          | 19.4      | 39.5      | 50.3         |\\n| IMAGEBIND       | A          | ✓          | 6.8       | 18.5      | 27.2         |\\n| IMAGEBIND       | A+V        | ✗          | 36.8      | 61.8      | 70.0         |\\n\\npervised'. IMAGEBIND's strong performance on all three benchmarks validates its ability to align the audio and text modalities using images as a bridge.\\n\\nText to audio and video retrieval. We use the MSR-VTT 1k-A benchmark to evaluate the text to audio and video retrieval performance in Table 4. Only using audio, IMAGEBIND achieves strong emergent retrieval performance compared to the video retrieval performance of prior work like MIL-NCE. The text to video performance for our model is strong (36.1% R@1 in Table 2) as it uses OpenCLIP's vision and text encoders and outperforms many prior methods. However, combining the audio and video modalities further boosts performance showing the utility of IMAGEBIND's features over an already strong retrieval model.\\n\\n## 4.3. Few-shot classification\\n\\nWe now evaluate the label-efficiency of IMAGEBIND by evaluating on few-shot classification. We use the audio and depth encoders from IMAGEBIND and evaluate them on audio and depth classification respectively in Figure 3. For ≥ 1-shot results, we follow [50, 60] and train linear classifiers on fixed features (details in Appendix B).\\n\\nOn few-shot audio classification (Figure 3 left), we compare with (1) self-supervised AudioMAE model trained\\n\\nFigure 5. Object detection with audio queries. Simply replacing Detic's CLIP-based 'class' embeddings with our audio embeddings leads to an object detector promptable with audio. This requires no re-training of any model.\\n\\n<!-- image -->\\n\\nFigure 3. Few-shot classification on audio and depth. Wereport the emergent zero-shot classification performance on each benchmark (denoted by ⋆ ). We train linear classifiers on fixed features for the ≥ 1 -shot case. (Left) In all settings, IMAGEBIND outperforms the self-supervised AudioMAE model. IMAGEBIND even outperforms a supervised AudioMAE model upto 4 shot learning showing its strong generalization. (Right) We compare with the MultiMAE model trained with images, depth, and semantic segmentation masks. IMAGEBIND outperforms MultiMAE across all few-shot settings on few-shot depth classification.\\n\\n<!-- image -->\\n\\non audio from Audioset and (2) a supervised AudioMAE model finetuned on audio classification. Both baselines use the same capacity ViT-B audio encoder as IMAGEBIND. IMAGEBIND significantly outperforms the AudioMAE model on all settings with gains of ∼ 40%accuracy in top-1 accuracy on ≤ 4-shot classification. IMAGEBIND also matches or outperforms the supervised model on ≥ 1shot classification. IMAGEBIND's emergent zero-shot performance surpasses the supervised ≤ 2-shot performance.\\n\\nFor few-shot depth classification, we compare with the multimodal MultiMAE ViT-B/16 model trained on images, depth, and semantic segmentation data. IMAGEBIND significantly outperforms MultiMAE across all the few-shot settings. Altogether, these results show the strong generalization of IMAGEBIND audio and depth features trained with image alignment.\\n\\n## 4.4. Analysis and Applications\\n\\nMultimodal embedding space arithmetic. We study whether IMAGEBIND's embeddings can be used to compose information across modalities. In Figure 4, we show image retrievals obtained by adding together image and audio embeddings. The joint embedding space allows for us to compose two embeddings: e.g ., image of fruits on a table + sound of chirping birds and retrieve an image that contains both these concepts, i.e ., fruits on trees with birds. Such emergent compositionality whereby semantic content from different modalities can be composed will likely enable a rich variety of compositional tasks.\\n\\nWithout re-training, we can 'upgrade' existing vision models that use CLIP embeddings to use IMAGEBIND embeddings from other modalities such as audio.\\n\\nUpgrading text-based detectors to audio-based. Weuse a\\n\\nFigure 4. Embedding space arithmetic where we add image and audio embeddings, and use them for image retrieval. The composed embeddings naturally capture semantics from different modalities. Embeddings from an image of fruits + the sound of birds retrieves images of birds surrounded by fruits.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nDog barking\\n\\nKeyboard typing\\n\\nClock alarm\\n\\nSea waves\\n\\npretrained text-based detection model, Detic, and simply replace its CLIP-based 'class' (text) embeddings with IMAGEBIND's audio embeddings. Without training, this creates an 'audio'-based detector that can detect and segment objects based on audio prompts. As shown in Figure 5, we can prompt the detector with the barking sound of a dog to localize a dog.\\n\\nUpgrading text-based diffusion models to audio-based. Weuse a pretrained DALLE-2 diffusion model (private reimplementation) and replace its prompt embeddings by our audio embeddings. In Figure 1, we observe that we can repurpose the diffusion model to generate plausible images using different types of sounds.\\n\\n## 5. Ablation Study\\n\\nWeinvestigate various design choices for learning a joint embedding space for different modalities. Since the ablation experimental setup is similar to § 4, we only note the main differences (full details in Appendix C). We report results on the ESC fold-1 for the ablation study. We use a ViTB encoder for the image, audio, depth, and thermal modalities by default and train them for 16 epochs ( vs . 32 epochs\\n\\nFigure 6. Scaling the image encoder size while keeping the other modality encoders' size fixed. We measure the performance on the emergent zero-shot classification of depth, audio, thermal, and IMUmodalities. Scaling the image encoder significantly improves the zero-shot classification results suggesting that a stronger visual representation improves the 'binding' of modalities.\\n\\n<!-- image -->\\n\\nin § 4). For IMU we use a lightweight 6 layer encoder with 512 dimensional width and 8 heads, and train it for 8 epochs. The text encoder follows and is a twelve layer Transformer with a width of 512 dimensions. We initialize the image and text encoder from the CLIP model.\\n\\n## 5.1. Scaling the Image Encoder\\n\\nThe central idea in IMAGEBIND is aligning the embeddings of all modalities to image embeddings. Thus, the image embeddings plays a central role in the emergent alignment of unseen modalities and we study their effect on the emergent zero-shot performance. We vary the size of the image encoder and train an encoder for the depth, audio etc . modalities to match the image representation. To isolate the effect of the image representation, we fix the size of the other modality encoders. We use the pretrained CLIP (ViT-B and ViT-L) and OpenCLIP (ViT-H) image and text encoders for this experiment. Our results in Figure 6 show that IMAGEBIND's emergent zero-shot performance on all modalities improves with better visual features. For depth and audio classification, the stronger ViT-H vs . the ViT-B image encoder, provides a gain of 7% and 4% respectively. Thus, stronger visual features can improve recognition performance even on non-visual modalities.\\n\\n## 5.2. Training Loss and Architecture\\n\\nWe study the effect of the training design choices on the emergent zero-shot classification. We focus on two modalities with different characteristics - depth which is visual and spatial, and audio which is non-visual and has a temporal component. We found that studying these diverse modalities led to robust and transferable design decisions.\\n\\nContrastive loss temperature. We study the effect of the\\n\\ntemperature τ ( Eq 1) in Table 5a. We experiment with a learnable temperature initialized to 0 07 . (parametrized in the log-scale) following vs . various values of fixed temperatures. Unlike, we observe that a fixed temperature is best for depth, audio and IMU classification. Additionally, we see that a higher temperature is better for training the depth, thermal, and IMU encoders, whereas a lower temperature works best for the audio modality.\\n\\nProjection head. Wevary the projection head used for each encoder from a linear layer to an MLP with 768 hidden dimensions. The results in Table 5b show that a linear projection performs better for both modalities. This is in contrast to standard self-supervised methods like SimCLR whose performance improves with MLP projection heads.\\n\\nTraining epochs. We vary the number training epochs and report the classification performance in Table 5c. Longer training consistently improves the emergent zero-shot performance for both modalities across all datasets.\\n\\nData augmentation for paired images. During IMAGEBIND training, we augment images either using basic augmentation (cropping, color jitter) or strong augmentation that additionally applies RandAugment and RandErase. We specify the augmentation parameters in Appendix C. Stronger augmentation helps depth classification when training on the small number of (image, depth) pairs from the SUN RGB-D dataset. However, for audio, strongly augmenting the video makes the task too challenging, leading to a significant drop of 34% on ESC.\\n\\nDepth specific design choices. We vary the type of spatial crops used for training in Table 5e. Following CMC, we use two unaligned random crops from the corresponding image and depth pair vs . our default choice of using spatially aligned random crops. Contrary to CMC, we observe that random cropping severely degrades performance: more than 10% on SUN-D. Unlike vanilla self-supervised learning, our image representations learned from imagetext pairs are more semantic and thus spatially misaligned crops hurt performance. In Table 5f, we observe that RandomErase boosts performance on depth classification.\\n\\nAudio specific design choices. We train for video-audio alignment using temporally aligned samples or unaligned samples and measure the final performance in Table 5g. Similar to the depth classification observation, temporally aligned samples lead to better performance. Table 5h shows that using frequency masking augmentation for audio also provides a small boost in performance.\\n\\nCapacity of the audio and depth encoders and their impact of the classification performance is reported in Table 6. A smaller encoder for depth improves performance presumably because of the relatively small size of the (image, depth) dataset. Conversely, we observe that larger audio encoder improves the performance, particularly when paired with a high capacity image encoder.\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nTable 5. Training loss and architecture design decisions and their impact on emergent zero-shot classification. Settings for results in § 4 highlighted in gray. (a) A fixed temperature in the contrastive loss outperforms a learnable one for all modalities. (b) A linear projection head for computing the depth or audio embedding works better than an MLP head. (c) Longer training improves the zero-shot classification performance for both modalities. (d) Stronger image augmentation improves depth classification while basic augmentation significantly improves audio classification. (e, f) Using spatially aligned image and depth crops when training IMAGEBIND significantly improves performance. Similarly, RandErase augmentation is critical to good zero-shot classification on depth. (g, h) Temporally aligned audio and video matching gives improved performance and using frequency augmentation for audio gives a slight improvement.\\n\\n<!-- image -->\\n\\nTable 6. Capacity of the audio and depth encoders and their impact on performance. A stronger image encoder improves performance for both audio and depth tasks. As the number of (image, depth) pairs is small, a smaller encoder improves performance for depth. For audio classification, a larger encoder is better.\\n\\n| Image Encoder   |   Audio Encoder (ESC) ViT-S |   ViT-B |   Depth Encoder (SUN) ViT-S |   ViT-B |\\n|-----------------|-----------------------------|---------|-----------------------------|---------|\\n| ViT-B           |                        52.8 |    56.7 |                        30.7 |    26.7 |\\n| ViT-H           |                        54.8 |    60.3 |                        33.3 |    29.5 |\\n\\nTable 7. Effect of scaling batch size. We found the optimal batch size for contrastive loss varied by the modality. For image-depth task, a smaller batch size was better, likely due to the small size and limited diversity of the original dataset. For audio-video task where we have a lot more positive and negative audio-video pairs, using a large batch size lead to better results.\\n\\n| Batch size   |   512 |   1k |   2k |   4k |\\n|--------------|-------|------|------|------|\\n| NYU-D        |  47.3 | 46.5 | 43   | 39.9 |\\n| ESC          |  39.4 | 53.9 | 56.7 | 53.9 |\\n\\nEffect of batch size. In Table 7 we evaluate the effect of batch size on the representation learned. As shown, the batch size can vary across modalities depending on the size and complexity of the corresponding pretraining datasets.\\n\\nIMAGEBIND to evaluate pretrained vision models in Table 8. We initialize the vision encoder using a pretrained model and keep it fixed. We use image-paired data to align and train text, audio, and depth encoders (full details in Appendix B). Compared to the supervised DeiT model, the self-supervised DINO model is better at emergent zero-shot classification on both depth and audio modalities. Moreover, the emergent zero-shot performance is not correlated with the pure vision performance on ImageNet suggesting that these tasks measure different properties. IMAGEBIND can serve as a valuable tool to measure vision models' strength on multimodal applications.\\n\\nTable 8. IMAGEBIND as an evaluation tool. We initialize (and fix) the image encoder with different methods and align other modalities. IMAGEBIND measures the impact of visual features on multimodal tasks. † trained with IN1K supervision.\\n\\n|           | IN1K   |      |      |      |   VGGS ESC SUN-D NYU-D |\\n|-----------|--------|------|------|------|------------------------|\\n| DINO  | 64.4   | 17.2 | 44.7 | 26.8 |                   48.8 |\\n| DeiT | 74.4 † |  9.6 | 25   | 25.2 |                   48   |\\n\\n## 6. Discussion and Limitations\\n\\nIMAGEBIND is a simple and practical way to train a joint embedding space using only image alignment. Our method leads to emergent alignment across all modalities which can be measured using cross-modal retrieval and text-based zero-shot tasks. We enable a rich set of compositional multimodal tasks across different modalities, show a way to evaluate pretrained vision models for non-vision tasks and 'upgrade' models like Detic and DALLE-2 to use using audio. There are multiple ways to further improve IMAGEBIND. Our image alignment loss can be enriched by using other alignment data, for instance other modalities paired with text, or with each other ( e.g . audio with IMU). Our embeddings are trained without a specific downstream task, and thus lag the performance of specialist models. More research into adapting general purpose embeddings for each task, including structured prediction tasks such as detection will be beneficial. Finally, new benchmarks, e.g . our emergent zero-shot task to measure emergent abilities of multimodal models, would help create exciting new applications. Our model is a research prototype and cannot be readily used for real world applications ( Appendix F).\\n\\nAcknowledgements: Authors would like to thank Uriel Singer, Adam Polyak and Naman Goyal for their help with the DALLE-2 experiments, and the entire Meta AI team for many helpful discussions.\\n\\n## A. Datasets and Metrics\\n\\nAudioset (AS). This dataset is used for both training and evaluation. It contains 10s videos from YouTube annotated into 527 classes. It consists of 3 pre-defined splits, the balanced split with about 20K videos, test split with 18K videos, and an unbalanced training split with about 2M vidoes. For training , we use the 2M unbalanced set without any labels, and only use it for audio-video matching. For zero-shot evaluation in Table 2, we use the test set and compute logits for each class using the textual class names along with the templates as described later in Appendix B.3. The metric used is top-1 accuracy.\\n\\nESC-50 (ESC). We use this dataset for evaluating the learned representations in a zero-shot manner. The task here is 'Environmental Sound Classification' (ESC). It consists of 2000 5s audio clips classified into 50 classes. It has predefined 5 fold evaluation, each consisting of 400 test audio clips. In this work, we compute 0-shot predictions on the evaluation set for each fold and report the 5-fold average performance. For ablations we use only the first fold for computational ease. The metric used is top-1 accuracy.\\n\\nClotho (Clotho). This is a dataset of audio from the Freesound platform with textual descriptions. It consists of a dev and test set of 2893 and 1045 audio clips respectively, with each clip associated with 5 descriptions. We consider the text → audio retrieval task, and consider each of the 5 associated captions as a separate test query and retrieve from the set of audio clips. The metric used is recall@ K , where a given test query is assumed to be correctly solved if the ground truth audio is retrieved within the topK retrieved audio clips.\\n\\nAudioCaps (AudioCaps). This is a dataset of audiovisual clips from YouTube accompanied by textual descriptions. It consists of clips from the Audioset dataset as described earlier. We use the splits as provided in, 1 which removes clips that overlap with the VGGSound dataset. We end up with 48198 training, 418 validation and 796 test clips. We only use the test set for zero-shot evaluation of our model. The task is text → audio retrieval, and evaluation is performed using recall@ K .\\n\\nVGGSound(VGGS). This dataset contains about 200K video clips of 10s length, annotated with 309 sound classes consisting of human actions, sound-emitting objects and human-object interactions. We only use the audio from the test set (with 14073 clips) for 0-shot classification. The evaluation is done using the top-1 accuracy metric.\\n\\nSUNRGB-D(SUN). Weuse the registered RGB and Depth maps provided in the SUN RGB-D dataset train set ( ∼ 5K pairs) for training our model. We follow to post process the depth maps in two steps - 1) we use in-filled\\n\\ndepth values and 2) convert them to disparity for scale normalization. This dataset is only used in training, so we do not use any metadata or class labels.\\n\\nSUN Depth-only (SUN-D). We use only the ∼ 5K depth maps from the val split of the SUN RGB-D dataset and denote them as SUN Depth-only. This dataset is only used for evaluation and we do not use the RGB images. We process the depth maps similar to SUN RGB-D (in-filled depth, converted to disparity). We use the 19 scene classes in the dataset and use their class names for constructing the zero-shot classification templates.\\n\\nNYU-v2 Depth-only (NYU-D). We use the 794 val set depth maps from the NYU-v2 Depth-only dataset for evaluation only. We post-process the depth similar to SUN Depth-only. We use the 10 scene class names in the dataset. The 10th scene class, called 'other', correspond to 18 different semantic classes -['basement', 'cafe', 'computer lab', 'conference room', 'dinette', 'exercise room', 'foyer', 'furniture store', 'home storage', 'indoor balcony', 'laundry room', 'office kitchen', 'playroom', 'printer room', 'reception room', 'student lounge', 'study', 'study room'] . For zero-shot evaluation, we compute the cosine similarity of the 10th class as the maximum cosine similarity among these 18 classnames.\\n\\nLLVIP (LLVIP). The LLVIP dataset consists of RGB image and Thermal (infrared low-light) image pairs. The dataset was collected in an outdoor setting using fixed cameras observing street scenes and contains RGB images taken in a low-light paired with infrared images (8 ∼ 14um frequency). The RGB thermal pairs are registered in the dataset release. For training, we use the train set with 12025 RGB image and thermal pairs. For evaluation, we use the val set with 3463 pairs of RGB and thermal images. Since the original dataset is designed for detection, we post process it for a binary classification task. We crop out pedestrian bounding boxes and random bounding boxes (same aspect ratio and size as pedestrian) to create a balanced set of 15809 total boxes (7931 'person' boxes). For zero-shot classification, we use the following class names for the 'person' class -['person', 'man', 'woman', 'people'] , and ['street', 'road', 'car', 'light', 'tree'] for the background class.\\n\\nEgo4D (Ego4D). For the Ego4D dataset, we consider the task of scenario classification. There are 108 unique scenarios present in the 9,645 videos of the Ego4D dataset. We filter out all videos annotated with more than one scenario which yields 7,485 videos with a single scenario assigned. For each video, We select all time-stamps that contains a synchronized IMU signal as well as aligned narrations. We sample 5 second clips around each time-stamp. The dataset is split randomly such that we have 510,142 clips for train-\\n\\ning, and 68,865 clips for testing. During training we only use the video frames and their corresponding IMU signal. We use the test split to measure zero-shot scenario classification performance, where each clip of IMU signal is assigned the video-level scenario label as its ground-truth.\\n\\n## A.1. Data Representations\\n\\nWe use the standard RGB and RGBT representations for images and videos . For videos, we use 2-frame clips, inspired from recent work on ViT-style video architectures [16, 71], where a video patch is 2 × 16 × 16 ( T × × H W ). We inflate the visual encoder's weights to work with spatiotemporal patches and and at inference time we aggregate features over multiple 2-frame clips. Hence, we can use models trained on image-text data directly on videos.\\n\\nWe used a single-channel image for the thermal data since it is the natural form in which current infrared thermal sensors return data. For single-view depth , we experimented with different encodings - absolute depth as returned by sensors like the Kinect, inverse depth, disparity, and HHA [25, 26]. Overall, we found that disparity representation (which is a single-channel image) worked the best. For audio we use the raw waveform processed into mel-spectrograms, as described in the main text. For IMU we use a 6 × T tensor to represent the sequence of IMU sensor readings over time.\\n\\n## B. Evaluation details\\n\\nWe now describe the evaluation setups used in this work.\\n\\n## B.1. Inference implementation details\\n\\nAudio/Video: For both these temporal modalities (whether operated upon together during pre-training or separately during inference), we sample fixed length clips to operate on. During training, we randomly sample a clip, typically 2s in length. At inference time, we uniformly sample multiple clips to cover the full length of the input sample. For instance, for 5s ESC videos, we would sample ⌈ 5 2 ⌉ = 3 clips. For video clips, we sample a fixed number of frames from each clip. For audio, we process each raw audio waveform by sampling it at 16KHz followed by extracting a log mel spectrogram with 128 frequency bins using a 25ms Hamming window with hop length of 10ms. Hence, for a t second audio we get a 128 × 100 t dimensional input.\\n\\nIMU: For IMU, we sample fixed length clips of 5 seconds, centered around time-stamps that are aligned with narrations. For each clip, we get a 6 × 2000 dimensional input and we measure the zero-shot performance for scenario classification using each clip as an independent testing sample.\\n\\n## B.2. Few-shot evaluation details\\n\\nFor the few-shot results in Figures 3 using the ESC and SUN datasets, we sampled k training samples per class,\\n\\nwhere k ∈ { 1 2 4 8 , , , } . We fix the k samples such that our model and the baselines use exactly the same samples during training. For all few-shot evaluations, including the baselines, we freeze the encoder parameters and only train a linear classifier.\\n\\nAudio: For audio few-shot training with ESC, our model and the baselines are trained using AdamW with a learning rate of 1 6 . × 10 -3 and weight decay of 0 05 . for 50 epochs. Depth: For depth few-shot training with SUN, our model and the baselines are trained using AdamW with a learning rate of 10 -2 and no weight decay for 60 epochs.\\n\\n## B.3. Zero-shot evaluation details\\n\\nQuery Templates. For all evaluations, we use the default set of templates from CLIP. 2 Note that we use the same templates for non visual modalities like audio and depth as well since we only use semantic/textual supervision associated with images.\\n\\n## B.4. Qualitative evaluation details\\n\\nCross-modal nearest neighbors. We perform the retrieval on the embedding feature after temperature scaling. The nearest neighbors are computed using cosine distance. In Figure 1, we show retrievals for audio from ESC, image retrievals from IN1K and COCO, depth from SUN-D, and text from AudioCaps.\\n\\nEmbedding arithmetic. For arithmetic, we again use the embedding features after temperature scaling. We ℓ 2 normalize the features and sum the embeddings after scaling them by 0 5 . . We use the combined feature to perform nearest neighbor retrieval using cosine distance, as described above. In Figure 1, we show combination of images and audio from IN1K and ESC, and show retrievals from IN1K. Audio → Image Generation. For generating images form audio clips, we rely on an in-house reproduced implementation of DALLE-2. In DALLE-2, to produce images from text prompts, the image generation model relies on text embeddings produced by the pre-trained CLIP-L/14 text encoder. Since IMAGEBIND naturally aligns CLIP'sembedding space to that of other modalities proposed in the paper, we can upgrade the DALLE-2 model to generate images by prompting it with these new unseen modalities. We achieve zero-shot audio to image generation with DALLE-2 by simply using the temperature-scaled audio embeddings generated by IMAGEBIND's audio encoder as a proxy for the CLIP's text embeddings in the DALLE-2's image generation model.\\n\\nDetecting objects using audio. We extract all audio descriptors from the validation set of ESC using an IMAGEBIND ViT-B/32 encoder, yielding 400 descriptors in total. We use an off-the-shelf CLIP-based Detic model and\\n\\nuse the audio descriptors as the classifier for Detic in place of CLIP text-based 'class' embeddings. We use a score threshold of 0.9 for the qualitative results in Figure 5.\\n\\n## C. Pretraining details\\n\\n## C.1. Best setup\\n\\nIn Table 9 we detail the hyperparameters used to pretrain each of the models reported in Table 4. Our experiments were done on 32GB V100 or 40GB A100 GPUs.\\n\\nTable 9. Pretraining hyperparameters\\n\\n| Config               | AS                     | SUN                    | LLVIP                  | Ego4D                  |\\n|----------------------|------------------------|------------------------|------------------------|------------------------|\\n| Vision encoder       | ViT-Huge               | ViT-Huge               | ViT-Huge               | ViT-Huge               |\\n| embedding dim.       | 768                    | 384                    | 768                    | 512                    |\\n| number of heads      | 12                     | 8                      | 12                     | 8                      |\\n| number of layers     | 12                     | 12                     | 12                     | 6                      |\\n| Optimizer            | AdamW                  | AdamW                  | AdamW                  | AdamW                  |\\n| Optimizer Momentum   | = 0 9 . , β 2 = 0 95 . | = 0 9 . , β 2 = 0 95 . | = 0 9 . , β 2 = 0 95 . | = 0 9 . , β 2 = 0 95 . |\\n| Peak learning rate   | 1.6e-3                 | 1.6e-3                 | 5e-4                   | 5e-4                   |\\n| Weight decay         | 0.2                    | 0.2                    | 0.05                   | 0.5                    |\\n| Batch size           | 2048                   | 512                    | 512                    | 512                    |\\n| Gradient clipping    | 1.0                    | 1.0                    | 5.0                    | 1.0                    |\\n| Warmup epochs        | 2                      | 2                      | 2                      | 2                      |\\n| Sample replication   | 1.25                   | 50                     | 25                     | 1.0                    |\\n| Total epochs         | 64                     | 64                     | 64                     | 8                      |\\n| Stoch. Depth    | 0.1                    | 0.0                    | 0.0                    | 0.7                    |\\n| Temperature          | 0.05                   | 0.2                    | 0.1                    | 0.2                    |\\n| Augmentations:       |                        |                        |                        |                        |\\n| RandomResizedCrop    |                        |                        |                        |                        |\\n| size                 |                        | 224px                  | 224px                  |                        |\\n| interpolation        |                        | Bilinear               | Bilinear               |                        |\\n| RandomHorizontalFlip |                        | p = 0 5 .              | p = 0 5 .              |                        |\\n| RandomErase          |                        | p = 0 25 .             | p = 0 25 .             |                        |\\n| RandAugment          |                        | 9/0.5                  | 9/0.5                  |                        |\\n| Color Jitter         |                        | 0.4                    | 0.4                    |                        |\\n| Frequency masking    | 12                     |                        |                        |                        |\\n\\nContrastive loss batch size vs . modalities. While contrastive losses do require larger batch size, this requirement didn't increase with the number of modalities. As noted in Appendix B, our experiments (Table 2) sample a minibatch of one pair of modalities at a time: batch size of 2K for (video, audio), and 512 for (image, depth), (image, thermal), and (video, IMU). These batch sizes are smaller than the &gt; 32K batch sizes used in prior work [10, 60].\\n\\nCombining modalities. In Table 4, we show results with combining the audio and video modalities. We combine them by extracting embeddings from both modalities per sample and computing a linear combinations of those embeddings. We used a weight of 0.95 for video and 0.05 for audio for this combination, which was found to perform the best.\\n\\nFigure 7. IMU retrievals. Given a text query, we show some IMU retrievals and corresponding video frames.\\n\\n<!-- image -->\\n\\n## C.2. Ablation setup\\n\\nThe following setup was used for our evaluations in § 5. Different from the best setup, all ablation experiments uses ViT-Base both for the vision and the modality-specific encoders. The models are trained for 16 epochs, unless mentioned otherwise.\\n\\nFor Table 5b, the differences between the linear and MLP heads are detailed below: The MLP head did not improve performance in our experiments.\\n\\n| Linear   | Linear(in dim, out dim)                      |\\n|----------|----------------------------------------------|\\n| MLP      | Linear(in dim, in dim), GELU, Linear(in dim, |\\n\\n## D. Additional Results\\n\\nQualitative results. Weshow additional results (along with audio) in the accompanying video.\\n\\nPractical applications of disparate modalities. In general, a shared embedding space enables a variety of different cross-modal search and retrieval applications. e.g ., since IMU sensors are ubiquitous (in phones, AR/VR headsets, health trackers), IMAGEBIND can allow a user to search an IMU database using text queries (without training with IMU-text pairs). IMU-based text search has applications in healthcare/activity search. For instance, in Figure 7 we show examples of IMU (and accompanying video) retrieval given textual search query. The retrieved IMU sample, shown as 3-channel Accelerometer (Acc) and Gyroscope (Gyro) recording, matches the text query.\\n\\n## E. Additional Ablations\\n\\nDesign choices in losses. Since the modality-specific encoders are trained to align with a frozen image encoder, we tried using a ℓ 2 regression objective. For ZS SUN top-1 accuracy, we observed that regression led to good performance as the sole objective (25.17%) or jointly with contrastive (29.04%). However, it did not improve over using only the contrastive objective (31.74%).\\n\\n## F. Ethical considerations\\n\\nIMAGEBIND learns a joint embedding for multiple modalities. Such an embedding is intended to associate semantically related concepts from different modalities. However, such an embedding may also create unintentional associations. Thus, joint embedding models, including IMAGEBIND must be studied carefully with a lens towards measuring such associations, and their implications. IMAGEBIND leverages the image-text embeddings learned by a pretrained model on large web-based data which has biases as documented in different studies. For learning joint embeddings for other modalities such as audio, thermal, depth, and IMU we leverage datasets mentioned in Appendix A. These joint embeddings are thus limited to the concepts present in the datasets. For example, the thermal datasets we used are limited to outdoor street scenes, while the depth datasets are limited to indoor scenes.\",\n",
              " 'similarity': 0.42020664894250837}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from graph import graph\n",
        "\n",
        "full_content = open(\"2305.05665v2.txt\").read()\n",
        "\n",
        "graph.invoke(\n",
        "    {\n",
        "        \"sentence\": \"In multilingual neural machine translation, a similar phenomenon to the emergence behavior of IMAGEBIND is commonly observed and utilized: if languages are trained in the same latent space through learned implicit bridging, translation can be done between language pairs on which no paired data is provided\",\n",
        "        \"full_content\": full_content,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "retriever = ArxivRetriever(top_k_results=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='In the past, the rapidly evolving field of sound classification greatly\n",
            "benefited from the application of methods from other domains. Today, we observe\n",
            "the trend to fuse domain-specific tasks and approaches together, which provides\n",
            "the community with new outstanding models.\n",
            "  In this work, we present an extension of the CLIP model that handles audio in\n",
            "addition to text and images. Our proposed model incorporates the ESResNeXt\n",
            "audio-model into the CLIP framework using the AudioSet dataset. Such a\n",
            "combination enables the proposed model to perform bimodal and unimodal\n",
            "classification and querying, while keeping CLIP's ability to generalize to\n",
            "unseen datasets in a zero-shot inference fashion.\n",
            "  AudioCLIP achieves new state-of-the-art results in the Environmental Sound\n",
            "Classification (ESC) task, out-performing other approaches by reaching\n",
            "accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.\n",
            "Further it sets new baselines in the zero-shot ESC-task on the same datasets\n",
            "(68.78% and 69.40%, respectively).\n",
            "  Finally, we also assess the cross-modal querying performance of the proposed\n",
            "model as well as the influence of full and partial training on the results. For\n",
            "the sake of reproducibility, our code is published.' metadata={'Entry ID': 'http://arxiv.org/abs/2106.13043v1', 'Published': datetime.date(2021, 6, 24), 'Title': 'AudioCLIP: Extending CLIP to Image, Text and Audio', 'Authors': 'Andrey Guzhov, Federico Raue, Jörn Hees, Andreas Dengel'}\n",
            "--------------------------------------------------\n",
            "page_content='An audio-visual event (AVE) is denoted by the correspondence of the visual\n",
            "and auditory signals in a video segment. Precise localization of the AVEs is\n",
            "very challenging since it demands effective multi-modal feature correspondence\n",
            "to ground the short and long range temporal interactions. Existing approaches\n",
            "struggle in capturing the different scales of multi-modal interaction due to\n",
            "ineffective multi-modal training strategies. To overcome this limitation, we\n",
            "introduce AVE-CLIP, a novel framework that integrates the AudioCLIP pre-trained\n",
            "on large-scale audio-visual data with a multi-window temporal transformer to\n",
            "effectively operate on different temporal scales of video frames. Our\n",
            "contributions are three-fold: (1) We introduce a multi-stage training framework\n",
            "to incorporate AudioCLIP pre-trained with audio-image pairs into the AVE\n",
            "localization task on video frames through contrastive fine-tuning, effective\n",
            "mean video feature extraction, and multi-scale training phases. (2) We propose\n",
            "a multi-domain attention mechanism that operates on both temporal and feature\n",
            "domains over varying timescales to fuse the local and global feature\n",
            "variations. (3) We introduce a temporal refining scheme with event-guided\n",
            "attention followed by a simple-yet-effective post processing step to handle\n",
            "significant variations of the background over diverse events. Our method\n",
            "achieves state-of-the-art performance on the publicly available AVE dataset\n",
            "with 5.9% mean accuracy improvement which proves its superiority over existing\n",
            "approaches.' metadata={'Entry ID': 'http://arxiv.org/abs/2210.05060v1', 'Published': datetime.date(2022, 10, 11), 'Title': 'AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization', 'Authors': 'Tanvir Mahmud, Diana Marculescu'}\n",
            "--------------------------------------------------\n",
            "page_content='Visual sound source localization poses a significant challenge in identifying\n",
            "the semantic region of each sounding source within a video. Existing\n",
            "self-supervised and weakly supervised source localization methods struggle to\n",
            "accurately distinguish the semantic regions of each sounding object,\n",
            "particularly in multi-source mixtures. These methods often rely on audio-visual\n",
            "correspondence as guidance, which can lead to substantial performance drops in\n",
            "complex multi-source localization scenarios. The lack of access to individual\n",
            "source sounds in multi-source mixtures during training exacerbates the\n",
            "difficulty of learning effective audio-visual correspondence for localization.\n",
            "To address this limitation, in this paper, we propose incorporating the text\n",
            "modality as an intermediate feature guide using tri-modal joint embedding\n",
            "models (e.g., AudioCLIP) to disentangle the semantic audio-visual source\n",
            "correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by\n",
            "predicting the class of sounding entities in mixtures. Subsequently, the\n",
            "textual representation of each sounding source is employed as guidance to\n",
            "disentangle fine-grained audio-visual source correspondence from multi-source\n",
            "mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables\n",
            "our framework to handle a flexible number of sources and exhibits promising\n",
            "zero-shot transferability to unseen classes during test time. Extensive\n",
            "experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets\n",
            "demonstrate significant performance improvements over state-of-the-art methods.\n",
            "Code is released at https://github.com/enyac-group/T-VSL/tree/main' metadata={'Entry ID': 'http://arxiv.org/abs/2404.01751v2', 'Published': datetime.date(2024, 7, 7), 'Title': 'T-VSL: Text-Guided Visual Sound Source Localization in Mixtures', 'Authors': 'Tanvir Mahmud, Yapeng Tian, Diana Marculescu'}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query = \"AudioCLIP adds audio as an additional modality into a CLIP framework, enabling zero-shot audio classification.\"\n",
        "docs = retriever.invoke(query)\n",
        "for doc in docs:\n",
        "    print(doc)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.711873196241966\n",
            "0.5347228688692579\n",
            "0.5064669280999857\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "\n",
        "def calculate_similarity(page_content: str, query: str) -> float:\n",
        "    doc_embedding = embeddings.embed_query(page_content)\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    similarity = cosine_similarity([doc_embedding], [query_embedding]).item()\n",
        "    return similarity\n",
        "\n",
        "\n",
        "for doc in docs:\n",
        "    print(calculate_similarity(doc.page_content, query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def is_arxiv_identifier(query: str) -> bool:\n",
        "    pattern = r\"\\d{2}(0[1-9]|1[0-2])\\.\\d{4,5}(v\\d+|)|\\d{7}.*\"\n",
        "    for item in query[:300].split():\n",
        "        if not re.match(pattern, item) or re.match(pattern, item).group(0) != item:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "is_arxiv_identifier(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['http://arxiv.org/pdf/2106.13043v1', 'http://arxiv.org/pdf/2210.05060v1', 'http://arxiv.org/pdf/2404.01751v2']\n"
          ]
        }
      ],
      "source": [
        "links = [doc.metadata[\"Entry ID\"].replace(\"/abs/\", \"/pdf/\") for doc in docs]\n",
        "print(links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5819485057638034\n",
            "0.5515498948932065\n",
            "0.4616582145934405\n"
          ]
        }
      ],
      "source": [
        "from langchain_playground.Tools import webloader\n",
        "\n",
        "content_list = []\n",
        "for link in links:\n",
        "    content = webloader(link)\n",
        "    content_list.append(content)\n",
        "\n",
        "\n",
        "for content in content_list:\n",
        "    print(calculate_similarity(content, query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Could not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention': [1/2] /usr/local/cuda-12.3/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/TH -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/teron/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
            "\u001b[31mFAILED: \u001b[0mms_deform_attn_cuda.cuda.o \n",
            "/usr/local/cuda-12.3/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/TH -isystem /home/teron/miniconda3/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/teron/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu:19:9: warning: #pragma once in main file\n",
            "   19 | #pragma once\n",
            "      |         ^~~~\n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu:19:9: warning: #pragma once in main file\n",
            "   19 | #pragma once\n",
            "      |         ^~~~\n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu(69): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n",
            "         ; at::ScalarType _st = ::detail::scalar_type(the_type); ; switch (_st) { case at::ScalarType::Double: { do { if constexpr (!at::should_include_kernel_dtype( at_dispatch_name, at::ScalarType::Double)) { if (!(false)) { ::c10::detail::torchCheckFail( __func__, \"/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\", static_cast<uint32_t>(69), (::c10::detail::torchCheckMsgImpl( \"Expected \" \"false\" \" to be true, but got false.  \" \"(Could this error message be improved?  If so, \" \"please report an enhancement request to PyTorch.)\", \"dtype '\", toString(at::ScalarType::Double), \"' not selected for kernel tag \", at_dispatch_name))); }; } } while (0); using scalar_t [[maybe_unused]] = c10::impl::ScalarTypeToCPPTypeT<at::ScalarType::Double>; return \n",
            "                                                      ^\n",
            "\n",
            "/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu(140): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n",
            "         ; at::ScalarType _st = ::detail::scalar_type(the_type); ; switch (_st) { case at::ScalarType::Double: { do { if constexpr (!at::should_include_kernel_dtype( at_dispatch_name, at::ScalarType::Double)) { if (!(false)) { ::c10::detail::torchCheckFail( __func__, \"/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\", static_cast<uint32_t>(140), (::c10::detail::torchCheckMsgImpl( \"Expected \" \"false\" \" to be true, but got false.  \" \"(Could this error message be improved?  If so, \" \"please report an enhancement request to PyTorch.)\", \"dtype '\", toString(at::ScalarType::Double), \"' not selected for kernel tag \", at_dispatch_name))); }; } } while (0); using scalar_t [[maybe_unused]] = c10::impl::ScalarTypeToCPPTypeT<at::ScalarType::Double>; return \n",
            "                                                      ^\n",
            "\n",
            "2 errors detected in the compilation of \"/home/teron/miniconda3/lib/python3.12/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\".\n",
            "ninja: build stopped: subcommand failed.\n",
            "\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
            "Could not load the custom kernel for multi-scale deformable attention: /home/teron/.cache/torch_extensions/py312_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from langchain_playground.Tools import webloader\n",
        "\n",
        "content = webloader(\"https://arxiv.org/pdf/2305.05665v2\")\n",
        "with open(\"2305.05665v2.txt\", \"w\") as f:\n",
        "    f.write(content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
